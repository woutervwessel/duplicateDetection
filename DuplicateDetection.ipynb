{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "ComputerScience07122021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/woutervwessel/duplicateDetection.git new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npExddaHU7Sh",
        "outputId": "7433a9cf-75ec-4491-d4e9-dc8d4ea82c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'new'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeFmt0m-WGC5"
      },
      "source": [
        "import pandas as pd\n",
        "import random as rd\n",
        "import json\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import sympy as sy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk7Tg44sWGC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2947bc1d-3bb8-47d8-96c6-2b9ddc2773c5"
      },
      "source": [
        "#Opening the .json which is read as a dictionary by python\n",
        "with open('new/TVs-all-merged.json') as f:\n",
        "    data = json.load(f)\n",
        "print(type(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping data to seperate elements and printing number of observations in full data set\n",
        "new_data = {}\n",
        "i = 1\n",
        "for key in data.keys():\n",
        "    for description in data[key]:\n",
        "        new_data[i] = description\n",
        "        i+=1\n",
        "print(len(new_data.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_40ZtKUl5ym",
        "outputId": "989ab46f-b9fa-4c9e-8c2a-ffb93c760b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkeKmd6PWGDC"
      },
      "source": [
        "#create set of titles \n",
        "def createTitleSet(data, includeKVP = False, onlyModelID = True):\n",
        "  \"\"\"Returns list of models words from the titles and feature map of each product\n",
        "  param: data, dictionary with all product information.\n",
        "  \n",
        "  The cleaning works as follows:\n",
        "  We attach all features in the title\n",
        "  \"\"\"\n",
        "  if includeKVP == True:\n",
        "      for i in range(1,len(data)):\n",
        "        kvp = ' '\n",
        "        for key in data[i]['featuresMap'].keys():\n",
        "          kvp = kvp + ' ' + data[i]['featuresMap'][key]\n",
        "        data[i]['title'] = data[i]['title']+kvp\n",
        "\n",
        "  set_titles = {}\n",
        "  for key in data.keys():\n",
        "      set_titles[key] = data[key]['title']\n",
        "\n",
        "  #Define regex functions\n",
        "  regex = '[a-zA-Z]+[0-9]+[a-zA-Z0-9]*|[a-zA-Z0-9]*[0-9]+[a-zA-Z]+|[0-9]+[.][0-9]+[a-zA-Z]*'\n",
        "  modelID_regex = '[a-zA-Z0-9]*[0-9]+[a-zA-Z]+[0-9]+[a-zA-Z0-9]*|[a-zA-Z0-9]*[a-zA-Z]+[0-9]+[a-zA-Z]+[a-zA-Z0-9]+'\n",
        "  \n",
        "  #Clean data\n",
        "  new_title= set_titles.copy()\n",
        "  for key in new_title.keys():\n",
        "      new_title[key] = new_title[key].lower()\n",
        "      new_title[key] = new_title[key].replace(' inches','inch')\n",
        "      new_title[key] = new_title[key].replace(' inch','inch')\n",
        "      new_title[key] = new_title[key].replace('\"','inch')\n",
        "      new_title[key] = new_title[key].replace('\" ','inch ')\n",
        "      new_title[key] = new_title[key].replace('hertz','hz')\n",
        "      new_title[key] = new_title[key].replace(' hertz','hz')\n",
        "      new_title[key] = new_title[key].replace(' hz','hz')\n",
        "      new_title[key] = re.sub(\"[^a-zA-Z0-9\\s\\.]\",\"\",new_title[key])\n",
        "      \n",
        "      #Maintain only modelID formatted words as proposed in the paper if onlyModelID == True\n",
        "      if onlyModelID == True:\n",
        "            modelID_title = re.findall(modelID_regex, new_title[key])\n",
        "            if not modelID_title:\n",
        "                new_title[key] = re.findall(regex, new_title[key]) \n",
        "                new_title[key] = list(set(new_title[key]))\n",
        "            else:\n",
        "                new_title[key] = list(set(modelID_title))\n",
        "      #If onlyModelID == False, we maintain all model words from the titles in the model word list\n",
        "      else:\n",
        "            new_title[key] = re.findall(regex, new_title[key]) \n",
        "            new_title[key] = list(set(new_title[key])) \n",
        "\n",
        "  return new_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvouQEtVWGDG"
      },
      "source": [
        "def createBinaryMatrix(data):\n",
        "    \"\"\"\n",
        "    param: data, dictionary, length=|P| \n",
        "    param: set_titles, dictionary, lenght=|P|\n",
        "    Returns binary values, list, shape=(|MW|, |P|)\n",
        "    \"\"\"\n",
        "    \n",
        "    #Create model word list\n",
        "    data_conc = sum(data.values(), [])\n",
        "    MW = sorted(set(data_conc), key=data_conc.index) #extracts unique model words \n",
        "\n",
        "    #create binary vector \n",
        "    #values are set to 1 if model word is present in title\n",
        "    binary_matrix = np.zeros((len(MW),len(data)),dtype=int)\n",
        "    p=0\n",
        "    for value in data.values():\n",
        "        w=0\n",
        "        for word in MW:\n",
        "            if word in value:\n",
        "                binary_matrix[w,p]=1\n",
        "            w= w+1\n",
        "        p=p+1\n",
        "    return binary_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv0sWUvGlGkl"
      },
      "source": [
        "#MinHash algorithm\n",
        "def createHashValues(lengthBinary, numHashes):\n",
        "  \"\"\"\n",
        "  This function creates hash values by permutating lengthBinary rows, numHashes times, so that we obtain a hash matrix of size lengthBinary times numHashes\n",
        "  param: lengthBinary, integer = |new_title|\n",
        "  param: numHashes, integer number of hash function\n",
        "  \"\"\"\n",
        "  mylist = []\n",
        "  hashval = list(range(1,lengthBinary+1))\n",
        "  hashind = list(range(1,lengthBinary+1))\n",
        "  for i in range(numHashes):\n",
        "    rd.shuffle(hashind)\n",
        "    dicto = dict(zip(hashind,hashval))\n",
        "    mylist.append(dicto)\n",
        "  return mylist\n",
        "  \n",
        "def createSignature(binary_matrix,hashValues,numHashes):\n",
        "  \"\"\"\n",
        "  Returns a list in list containing signatures for each product (Size: #Products x #numHashes)\n",
        "  \"\"\"\n",
        "  signatures=[]\n",
        "  for product in range(binary_matrix.shape[1]):\n",
        "    sig_val = []\n",
        "    for i in range(numHashes): #N signature values\n",
        "      for hash_val in range(len(binary_matrix)): #Find minHashValue for i-th signature value, goes from 0 to 1299\n",
        "        #print(product,i,hash_val)\n",
        "        index =  hashValues[i].get(hash_val+1) #add 1 to hash value as we go from 1 to 1300 and not 0 to 1299\n",
        "        if (binary_matrix[index-1,product]) == 1:\n",
        "          sig_val.append(hash_val+1)\n",
        "          break\n",
        "    signatures.append(sig_val)\n",
        "  return signatures\n",
        "\n",
        "def minHashing(binary_matrix, numHashes):\n",
        "  \"\"\"\n",
        "  Returns signatures for each product\n",
        "  \"\"\"\n",
        "  hashValues = createHashValues(len(binary_matrix),numHashes)\n",
        "  signatures = createSignature(binary_matrix,hashValues,numHashes)\n",
        "  return signatures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36nGWRurI0fy"
      },
      "source": [
        "#LSH\n",
        "def hashSignaturesToBuckets(data, signatures, b):\n",
        "    \"\"\"\n",
        "    Hashes column bands of each signature to a buckets of that band \n",
        "    : param signatures: array-like, shape=(|P|, n)\n",
        "    returns dictionary with buckets for each band \n",
        "    \"\"\"\n",
        "    n = len(signatures[0])\n",
        "    assert n % b == 0  \n",
        "    r = int(n/b)   #number of rows in each band\n",
        "    product_keys = list(data.keys()) #to get right product key with index\n",
        "    \n",
        "    bucket_bands =[]\n",
        "    for i in range(b): #construct for each band a empty list of buckets \n",
        "        bucket_bands.append({})\n",
        "\n",
        "    buckets=[]\n",
        "    sign_idx=0\n",
        "\n",
        "    #goes trough all product signatures\n",
        "    for signature in signatures: \n",
        "        bands= signatureToBands(signature, r)\n",
        "\n",
        "        #goes trough each band i of a product signature\n",
        "        for i, band in enumerate(bands.astype(str)): \n",
        "            band = ','.join(band)\n",
        "            if band not in bucket_bands[i].keys():\n",
        "                bucket_bands[i][band]=[]\n",
        "            bucket_bands[i][band].append(product_keys[sign_idx])\n",
        "        sign_idx +=1\n",
        "    return bucket_bands\n",
        "\n",
        "\n",
        "def signatureToBands(signature, r):\n",
        "    \"\"\"\n",
        "    Creates bands of length r for each signature vector\n",
        "    : param signature: array-like, shape=(1, |P|)\n",
        "    b: number of bands \n",
        "    returns list of subvectors for signature\n",
        "    \"\"\"\n",
        "    bands=[]\n",
        "    for i in range(0,len(signature), r):#step size r \n",
        "        bands.append(signature[i:i+r])\n",
        "    return np.stack(bands)\n",
        "\n",
        "\n",
        "def candidates(bucket_bands):\n",
        "    \"\"\"\n",
        "    Returns list of candidate neigbors\n",
        "    \"\"\"\n",
        "    candidates = []\n",
        "    for bucket_band in bucket_bands:\n",
        "        for bucket in bucket_band.keys():\n",
        "            products_in_bucket = bucket_band[bucket]\n",
        "            if len(products_in_bucket) > 1:\n",
        "                candidates.extend(combinations(products_in_bucket, 2))\n",
        "    return list(set(candidates)) #returns unique pairs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEPZgDQbzIBm"
      },
      "source": [
        "def SSM(candidate_pairs, new_title, sim_tres):\n",
        "    \"\"\"\n",
        "    SSM calculates the similarity between two candidate product pairs produced by LSH\n",
        "    It computes the percentage of matching modelwords in the title\n",
        "    When this is above threshold (sim_tres), a pair is seen as duplicate\n",
        "    All pairs that are seen as duplicates are returned as a list called found_pairs\n",
        "    \"\"\"\n",
        "    found_pairs = [];\n",
        "    for i in range(len(candidate_pairs)):\n",
        "        prod1 = candidate_pairs[i][0]\n",
        "        prod2 = candidate_pairs[i][1]\n",
        "        simscore = len(list(set(new_title[prod1]).intersection(set(new_title[prod2]))))/len(list(set(new_title[prod1]).union(set(new_title[prod2]))))\n",
        "        if simscore > sim_tres:\n",
        "            found_pairs.append(candidate_pairs[i])\n",
        "    return found_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0gfjl4_NI-L"
      },
      "source": [
        "def sampleData(data, n_samples):\n",
        "  \"\"\"Samples data\n",
        "  param: data, dictionary with all product information, dictionary\n",
        "  param: n_samples, number of samples to draw, integer \n",
        "  \"\"\"\n",
        "  sample_d = {}\n",
        "  for i in range(1, n_samples+1):\n",
        "    draw = rd.randint(1,len(data))\n",
        "    sample_d[i] = data[draw]\n",
        "  return sample_d\n",
        "\n",
        "def truePositive(candidate_pairs):\n",
        "  \"\"\"Return number of true postive pairs\n",
        "  param: candidate_pairs, list of canidates pairs obtained from LSH\"\"\"\n",
        "  TP=0\n",
        "  for candidate_pair in candidate_pairs:\n",
        "    if data[candidate_pair[0]]['modelID']==data[candidate_pair[1]]['modelID']:\n",
        "      TP +=1\n",
        "  return TP\n",
        "\n",
        "def totalAmountDuplicates(data):\n",
        "  \"\"\"Returns number of duplicates in the data\"\"\"\n",
        "  modelIDs = {}\n",
        "  for key in data.keys():\n",
        "    modelIDs[key] = data[key]['modelID']\n",
        "\n",
        "  rev_dict = {} #dictionary with key: ModelID and value: {indices with that modelID}\n",
        "  for key, modelID in modelIDs.items():\n",
        "    rev_dict.setdefault(modelID, set()).add(key)\n",
        "\n",
        "  duplicates = []\n",
        "  for value in rev_dict.values():\n",
        "    if len(value) > 1:\n",
        "      duplicates.extend(combinations(value, 2))\n",
        "\n",
        "  return duplicates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeMeasures(duplicates, candidate_pairs, found_pairs):\n",
        "    \"\"\"Returns PQ, PC, F1* and F1\"\"\"\n",
        "    #make sets from results to apply intersection/union functions\n",
        "    duplicates = set(duplicates)\n",
        "    candidate_pairs = set(candidate_pairs)\n",
        "    found_pairs = set(found_pairs)\n",
        "\n",
        "    total_duplicates = len(duplicates)\n",
        "\n",
        "    #measures LSH\n",
        "    total_duplicates_found = len(candidate_pairs)\n",
        "    Df = len(candidate_pairs.intersection(duplicates))\n",
        "\n",
        "    #measures MSM\n",
        "    TP = len(found_pairs.intersection(duplicates))\n",
        "    FP = len(found_pairs) - TP\n",
        "    FN = total_duplicates - TP\n",
        "\n",
        "    #metrics\n",
        "    PQ = Df / total_duplicates_found\n",
        "    PC = Df / total_duplicates\n",
        "    F1_star = (2 * PQ * PC)/ (PQ+PC)\n",
        "    precision = TP / len(found_pairs)\n",
        "    recall = TP / (TP + FN)\n",
        "    F1 = (2 * recall * precision ) / (recall + precision)\n",
        "  \n",
        "\n",
        "    return PQ, PC, F1_star, F1"
      ],
      "metadata": {
        "id": "WCOy67nryjWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x20SWuCWGhUl"
      },
      "source": [
        "### Perform bootstrap and compute evaluation metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bootstraps for LSH performance\n",
        "bootstraps = 5\n",
        "iter = 0\n",
        "settings =[]\n",
        "T = []\n",
        "\n",
        "n_samples = int(len(new_data)*0.6) #\n",
        "n = 720 #number of hash functions for signature matrix\n",
        "sim_tres = 0.6 #similarity treshold for SSM\n",
        "\n",
        "for r in [1,2,3,4,5,6,8,10,15,20,30,48]: #Manually configured based on threshold values\n",
        "    if (n % r) == 0:\n",
        "        b = n/ r\n",
        "        t = (1/b) ** (1/r)\n",
        "        settings.append([t,b,r])\n",
        "        T.append(t)\n",
        "        \n",
        "\n",
        "while iter != bootstraps:\n",
        "    #Sample data\n",
        "    data = sampleData(new_data, n_samples)\n",
        "    \n",
        "    #Create titles\n",
        "    set_titles = createTitleSet(data, includeKVP = False, onlyModelID = False)\n",
        "\n",
        "    #Start minhasing\n",
        "    binary_matrix = createBinaryMatrix(set_titles) \n",
        "    signatures = minHashing(binary_matrix, n)\n",
        "    \n",
        "    duplicates  = totalAmountDuplicates(data)\n",
        "    \n",
        "    #Metric values for all band values for bootstrap i \n",
        "    PQ_i  = [] \n",
        "    PC_i = []\n",
        "    F1star_i = []\n",
        "    F1_i = []\n",
        "    foc_i = []\n",
        "\n",
        "    for setting in settings:\n",
        "        #Apply LSH\n",
        "        bands =  int(setting[1])\n",
        "        bucket_bands = hashSignaturesToBuckets(set_titles, signatures, bands)\n",
        "        candidate_pairs= candidates(bucket_bands)\n",
        "        \n",
        "        #Apply SSM\n",
        "        found_pairs = SSM(candidate_pairs, set_titles, sim_tres)\n",
        "        \n",
        "        #Compute measures\n",
        "        PQ, PC, F1_star, F1 = computeMeasures(duplicates, candidate_pairs, found_pairs)\n",
        "        fraction_of_comparison = len(candidate_pairs) / len(list(combinations(data.keys(), 2)))\n",
        "     \n",
        "        PQ_i.append(PQ)\n",
        "        PC_i.append(PC)\n",
        "        F1star_i.append(F1_star)\n",
        "        F1_i.append(F1)\n",
        "        foc_i.append(fraction_of_comparison)\n",
        "        \n",
        "    if iter==0:\n",
        "        measures = np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)\n",
        "    else:\n",
        "        measures += np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)\n",
        "        \n",
        "    iter += 1\n",
        "\n",
        "#averaged measures with rows=(PQ, PC, F1star, F1) and columns=|settings|\n",
        "av_measures = measures / bootstraps\n",
        "print('\\n PQ:', av_measures[0], '\\n PC:', av_measures[1], '\\n F1_star', av_measures[2], '\\n F1', av_measures[3], '\\n fraction_of_comparisons', av_measures[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgeWY8PXzxJz",
        "outputId": "c9f30c35-367e-4ae1-b330-76d6b4453a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " PQ: [0.00059486 0.00068616 0.00352273 0.015061   0.0647036  0.16996872\n",
            " 0.31474104 0.40203562 0.47590361 0.48615385 0.51633987 0.51815182] \n",
            " PC: [0.50704225 0.50704225 0.47183099 0.42018779 0.39201878 0.38262911\n",
            " 0.37089202 0.37089202 0.37089202 0.37089202 0.37089202 0.3685446 ] \n",
            " F1_star [0.00118832 0.00137047 0.00699325 0.02907968 0.11107416 0.23537906\n",
            " 0.34051724 0.38583639 0.41688654 0.4207723  0.43169399 0.43072702] \n",
            " F1 [0.39303483 0.39303483 0.39303483 0.39303483 0.39303483 0.39303483\n",
            " 0.395      0.39949431 0.41688654 0.4207723  0.43169399 0.43072702] \n",
            " fraction_of_comparisons [7.66297845e-01 6.64333303e-01 1.20413379e-01 2.50817240e-02\n",
            " 5.44685988e-03 2.02384294e-03 1.05940475e-03 8.29374635e-04\n",
            " 7.00642185e-04 6.85869609e-04 6.45772616e-04 6.39441512e-04]\n"
          ]
        }
      ]
    }
  ]
}